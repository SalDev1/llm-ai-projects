{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHsssBgWM_l0"
      },
      "source": [
        "## Predict Product Prices\n",
        "\n",
        "### Week 7 Day 1\n",
        "\n",
        "An introduction to LoRA and QLoRA\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiThFU7pE7Hw"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d14UgrL83gcg"
      },
      "source": [
        "## A reminder of 2 important pro-tips for using Colab:\n",
        "\n",
        "**Pro-tip 1:**\n",
        "\n",
        "The top of every colab has some pip installs. You may receive errors from pip when you run this, such as:\n",
        "\n",
        "> gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
        "\n",
        "These pip compatibility errors can be safely ignored; and while it's tempting to try to fix them by changing version numbers, that will actually introduce real problems!\n",
        "\n",
        "**Pro-tip 2:**\n",
        "\n",
        "In the middle of running a Colab, you might get an error like this:\n",
        "\n",
        "> Runtime error: CUDA is required but not available for bitsandbytes. Please consider installing [...]\n",
        "\n",
        "This is a super-misleading error message! Please don't try changing versions of packages...\n",
        "\n",
        "This actually happens because Google has switched out your Colab runtime, perhaps because Google Colab was too busy. The solution is:\n",
        "\n",
        "1. Runtime menu >> Disconnect and delete runtime\n",
        "2. Reload the colab from fresh and Edit menu >> Clear All Outputs\n",
        "3. Connect to a new T4 using the button at the top right\n",
        "4. Select \"View resources\" from the menu on the top right to confirm you have a GPU\n",
        "5. Rerun the cells in the colab, from the top down, starting with the pip installs\n",
        "\n",
        "And all should work great - otherwise, ask me!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDyR63OTNUJ6",
        "outputId": "931d4c90-11f1-498b-c747-a273e70e5433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.2/908.2 MB\u001b[0m \u001b[31m708.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m619.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m114.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# pip installs\n",
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q requests bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0\n",
        "!pip install -q datasets requests peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yikV8pRBer9"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, set_seed\n",
        "from peft import LoraConfig, PeftModel\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuTX-xonNeOK"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "FINETUNED_MODEL = f\"ed-donner/pricer-2024-09-13_13.04.39\"\n",
        "\n",
        "# Hyperparameters for QLoRA Fine-Tuning\n",
        "\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 64\n",
        "# Setting up the target modules for LoRA adaptation.\n",
        "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JArT3QAQAjx"
      },
      "source": [
        "### Log in to HuggingFace\n",
        "\n",
        "If you don't already have a HuggingFace account, visit https://huggingface.co to sign up and create a token.\n",
        "\n",
        "Then select the Secrets for this Notebook by clicking on the key icon in the left, and add a new secret called `HF_TOKEN` with the value as your token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyFPZeMcM88v"
      },
      "outputs": [],
      "source": [
        "# Log in to HuggingFace\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJWQ0a3wZ0Bw"
      },
      "source": [
        "## Trying out different Quantization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElUPTnB28iNK"
      },
      "outputs": [],
      "source": [
        "# Load the Base Model without quantization\n",
        "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VcoS_DY9YSG"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6gJWw3r86KQ"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'base_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMemory footprint: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbase_model\u001b[49m.get_memory_footprint()\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m1e9\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m GB\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mNameError\u001b[39m: name 'base_model' is not defined"
          ]
        }
      ],
      "source": [
        "# This helps in understanding on how much memory the base mode is using us.\n",
        "# This will be around - 32GB (32 bit floats for each 8B parameters)\n",
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sVf8hf6S88C9"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'base_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbase_model\u001b[49m\n",
            "\u001b[31mNameError\u001b[39m: name 'base_model' is not defined"
          ]
        }
      ],
      "source": [
        "base_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv-00uzNFPOe"
      },
      "source": [
        "## Restart your session!\n",
        "\n",
        "In order to load the next model and clear out the cache of the last model, you'll now need to go to Runtime >> Restart session and run the initial cells (installs and imports and HuggingFace login) again.\n",
        "\n",
        "This is to clean out the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ycR0B4CzUSJ"
      },
      "outputs": [],
      "source": [
        "# Load the Base Model using 8 bit\n",
        "\n",
        "# Bites and Bytes configuration helps in specfiying what kind of quantization config we want + defining the reduction in precision of the model.\n",
        "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# We load the LLAMA 3.1 base model with the precision reduced from 32 bits to 8 bits.\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVErveOYFOXW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FDztdnv0RCq"
      },
      "outputs": [],
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvqOxYfk0RnY"
      },
      "outputs": [],
      "source": [
        "base_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb7-0OfSEycl"
      },
      "source": [
        "## Restart your session!\n",
        "\n",
        "In order to load the next model and clear out the cache of the last model, you'll now need to go to Runtime >> Restart session and run the initial cells (imports and HuggingFace login) again.\n",
        "\n",
        "This is to clean out the GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_O04fKxMMT-"
      },
      "outputs": [],
      "source": [
        "# Load the Tokenizer and the Base Model using 4 bit\n",
        "\n",
        "# The main thing it happens in\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,  # Load in th 4 bit quantization.\n",
        "    bnb_4bit_use_double_quant=True,  # It conducts two passes of the quantizating all of the weights and with that it reduces the memory by 10-20% (Which is good).\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16, # Using bfloat16 for computation usually increases the speed of the training.\n",
        "    bnb_4bit_quant_type=\"nf4\") # This helps in interpreting the 4 bit numbers in a better way.\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FfMJ2JbzEr3"
      },
      "outputs": [],
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjp1EHH10WTb"
      },
      "outputs": [],
      "source": [
        "base_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSpavkXo1KOI"
      },
      "outputs": [],
      "source": [
        "fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3kXoy3w1oMF"
      },
      "outputs": [],
      "source": [
        "print(f\"Memory footprint: {fine_tuned_model.get_memory_footprint() / 1e9:,.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2PL1nlM1tM1"
      },
      "outputs": [],
      "source": [
        "fine_tuned_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIaqo-gyBQRh"
      },
      "outputs": [],
      "source": [
        "# Each of the Target Modules has 2 LoRA Adaptor matrices, called lora_A and lora_B\n",
        "# These are designed so that weights can be adapted by adding alpha * lora_A * lora_B\n",
        "# Let's count the number of weights using their dimensions:\n",
        "\n",
        "# See the matrix dimensions above\n",
        "lora_q_proj = 4096 * 32 + 4096 * 32\n",
        "lora_k_proj = 4096 * 32 + 1024 * 32\n",
        "lora_v_proj = 4096 * 32 + 1024 * 32\n",
        "lora_o_proj = 4096 * 32 + 4096 * 32\n",
        "\n",
        "# Each layer comes to\n",
        "lora_layer = lora_q_proj + lora_k_proj + lora_v_proj + lora_o_proj\n",
        "\n",
        "# There are 32 layers\n",
        "params = lora_layer * 32\n",
        "\n",
        "# So the total size in MB is\n",
        "size = (params * 4) / 1_000_000\n",
        "\n",
        "# At the end, we are able to use these lower dimensional matrices to have an impact on the bigger architecture. \n",
        "\n",
        "# 109MB --> It is the size of the paramters that are fine tuning using the LoRA technique.\n",
        "print(f\"Total number of params: {params:,} and size {size:,.1f}MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKnCsfUQBG-P"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
