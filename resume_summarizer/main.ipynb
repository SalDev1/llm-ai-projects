{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c1d9d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b998a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching all the content inside a pdf, once the user uploads it to the Gradio UI.\n",
    "# Returns the array of pages from the pdf and open the pdf document.\n",
    "def summarize_content_pdf(file_path): \n",
    "    print(\"salman file_path is : \" , file_path)\n",
    "    doc  = fitz.open(file_path)\n",
    "    print(type(doc))  # <class 'fitz.fitz.Document'>\n",
    "\n",
    "    print(doc.page_count)  # Number of pages in the pdf file.\n",
    "\n",
    "    main_txt = \"\";\n",
    "    for page in doc : \n",
    "        text = page.get_text()\n",
    "        main_txt += text\n",
    "    \n",
    "    print(main_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0a81f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7877\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7877/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salman file_path is :  C:\\Users\\salma\\AppData\\Local\\Temp\\gradio\\5a26b57a51bb1180cddbca960b696260d2282810307b8694acc05f7ed49cee85\\AI Agents  LLM.pdf\n",
      "<class 'pymupdf.Document'>\n",
      "9\n",
      " \n",
      "AI Agents & LLM \n",
      " \n",
      "Resources & Links : Link1 \n",
      " \n",
      "If you want to play around with AI models that are open source in nature : Ollama allows \n",
      "you to run those defined models locally on your computer.  \n",
      " \n",
      "Once you run Ollama on your cmd , you can run it locally on the localhost : \n",
      "http://localhost:11434/  \n",
      " \n",
      "Let’s understand how to build a commercial project through an open source model we \n",
      "fetch from Ollama.  \n",
      "●​ LLama 3.2  \n",
      "●​ GPT-OSS 3.B \n",
      " \n",
      "Example 1 : Let’s build a Website Summarizer that summarizes the content with AI : \n",
      "Link \n",
      "●​ Writing a program that takes a website url .  \n",
      "●​ Scrapes the content from the website.  \n",
      "●​ Summarizes the content using the ChatGPT model (Frontier Models) \n",
      " \n",
      "What are Frontier Models ?  \n",
      "●​ Frontier models are meant to be closed-source in nature such as (ChatGPT, \n",
      "Claude and many more). \n",
      "●​ They are the highest scale of the models.  \n",
      " \n",
      "Closed-Source Frontier Models :  \n",
      "●​ GPT from OpenAI \n",
      "●​ Claude from Anthropic \n",
      "●​ Gemini from Google.  \n",
      "●​ Command R from Cohere. \n",
      "●​ Perplexity. \n",
      " \n",
      "Open-Source Frontier Models :  \n",
      "●​  Llama from Meta \n",
      "●​ Mixtral from Mistral \n",
      "●​ Qwen from Alibaba Cloud \n",
      "●​ Gemma from Google \n",
      " \n",
      "●​ Phi from Microsoft.  \n",
      " \n",
      "Three ways to use models :  \n",
      "●​ Chat Interfaces : ChatGPT \n",
      "●​ Cloud APIs  :  \n",
      "○​ Frameworks like LangChain  \n",
      "○​ Managed AI cloud services :  \n",
      "■​ Amazon BedRock \n",
      "■​ Google Vertex \n",
      "■​ Azure ML \n",
      "●​ Direct inference :  \n",
      "○​ With the huggingFace Transformers library \n",
      "○​ With Ollama to run locally.  \n",
      " \n",
      "Frontier models & End-user UIs :  \n",
      "●​ OpenAI :  \n",
      "○​ Models : GPT, o1  \n",
      "○​ Chat: ChatGPT \n",
      "●​ Anthropic :  \n",
      "○​ Models : Claude  \n",
      "○​ Chat : Claude  \n",
      "●​ Google :  \n",
      "○​ Models : Gemini  \n",
      "○​ Chat : Gemini Advance  \n",
      "●​ Cohere :  \n",
      "○​ Command R +  \n",
      "○​ Command R +  \n",
      "●​ Meta :  \n",
      "○​ Llama  \n",
      "○​ Meta.AI  \n",
      "●​ Perplexity :  \n",
      "○​ Models : Perplexity  \n",
      "○​ Search : Perplexity  \n",
      " \n",
      "Pros of such Frontier Models :  \n",
      "●​ Synthesizing information :  \n",
      "○​ Answering the question in depth with a structured, well researched answer \n",
      "and often including a summary.  \n",
      "●​ Fleshing out a skeleton :  \n",
      " \n",
      "○​ From a couple of notes, building out a well crafted email, or a blog post \n",
      "and iterating on it with you until perfect.  \n",
      "●​ Coding :  \n",
      "○​ The ability to write and debug code is remarkable, far overtaking Stack \n",
      "Overflow as the resource for engineers.  \n",
      " \n",
      "Limitations of Frontier models :  \n",
      "●​ Specialized domains : Most are not PhD level, but closing in.  \n",
      "●​ Recent events : Limited knowledge beyond training cut-off date.  \n",
      "●​ Can confidently make mistakes : Some curious blindspots.  \n",
      " \n",
      "Number of parameters in models (log scale) :  \n",
      "●​ Parameters == Are also defined as weights.  \n",
      "Eg :-  \n",
      "●​ GPT-1 : 117 Million parameters.  \n",
      "●​ GPT-2 : 1.5 Billion parameters.  \n",
      "●​ GPT-3 : 175 Billion parameters.  \n",
      "●​ GPT-4 : 1.76 Trillion parameters. \n",
      " \n",
      " \n",
      "Tokens :  \n",
      "●​ Tokens are individual units which get passed into a model.  \n",
      "A] In the early days, neural networks were trained at the character level (character by \n",
      "character). Small vocab, but expects too much from the network.  \n",
      "●​ Predict the next character in this sequence.  \n",
      " \n",
      "B] Then later, the models were trained based on each possible word.  \n",
      "●​ Predict the next word in this sequence.  \n",
      "●​ Much easier to learn from, but leads to enormous vocabs with rare words \n",
      "omitted.  \n",
      " \n",
      "C] The breakthrough was to work with chunks of words, called “tokens” (During the \n",
      "initial phases of GPT) \n",
      "●​ A middle ground; manageable vocab, and useful information for the neural \n",
      "network.  \n",
      "●​ In addition, it elegantly handles word stems.  \n",
      "●​ Chunks (Tokens)  → sometimes have a complete word or sometimes have a \n",
      "partial incomplete word.  \n",
      " \n",
      "And train the model in such a way where it takes a series of tokens and outputs tokens \n",
      "based on the tokens that were passed in the model already.  \n",
      "Example : https://platform.openai.com/tokenizer. \n",
      " \n",
      " \n",
      "●​ For common words, 1 word maps to 1 token + gap between words is also \n",
      "included as part of the word / token.  \n",
      " \n",
      " \n",
      "Example 2 :  \n",
      " \n",
      " \n",
      " \n",
      "Context Window :  \n",
      "●​ Max number of tokens that the model can consider when generating the next \n",
      "token.  \n",
      "●​ Includes the original input prompt, subsequent conversation, the latest input \n",
      "prompt and almost all the output prompt. (It forms a huge long prompt string \n",
      "when the user enters the message to the model + adds up other conservation \n",
      "prompts as the user moves deep into the conversations). \n",
      "●​ It governs how well the model can remember references, content and context.  \n",
      "●​ Particularly important for multi-shot prompting where the prompt includes \n",
      "examples, or for long conversations.  \n",
      "●​ Or questions on the complete works of Shakespare !  \n",
      "●​ Example : Some models are trained to handle 1.2M tokens in just a single prompt \n",
      "(Daamnn !!!!)  \n",
      " \n",
      "API costs:  \n",
      " \n",
      "●​ Chat interfaces typically have Pro plans with a monthly subscription. Rate-limited, \n",
      "but no per-usage charge. \n",
      "●​ APIs typically have no subscription, but charge per API call.  \n",
      "●​ The cost is based on the number of input tokens and the number of output \n",
      "tokens.  \n",
      " \n",
      "To compare different frontier models on standard benchmarks, Link \n",
      " \n",
      "One-Shot Prompting :- It helps us in dealing with the nuanced tasks by providing one \n",
      "example and once it’s trained on that example, it starts giving us structured outputs as \n",
      "desired. \n",
      "●​ This is an excellent use case for an LLM, because it requires nuanced \n",
      "understanding.  \n",
      " \n",
      "Structured Outputs → Forces the LLM to respond in a particular way.  \n",
      " \n",
      "Adversarial Conversation between ChatBots :  \n",
      "●​ Configuring two chatbots in mind with the API , we can conduct real-time \n",
      "conversation between two or more models.  \n",
      " \n",
      "Gradio UI :  \n",
      "●​ If you are looking for a way to introduce a UI which interacts with the frontier LLM \n",
      "models easily, Gradio python library helps you with it.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "yield in Python :  \n",
      "●​ It’s used inside a function to turn into a generator \n",
      "●​ Instead of returning from a function , we use the keyword yield = forms a \n",
      "generator fn which yields each element one by one instead of all at once. \n",
      "  \n",
      " \n",
      "Use of Prompts with our Assistant :  \n",
      " \n",
      " \n",
      "RAG :- It helps you in finding extra information that’s relevant to the prompt, and adding \n",
      "it into the context of the message sent to the LLM. \n",
      " \n",
      " \n",
      "AI Tools :  \n",
      "●​ Allows frontier models to connect with external functions  \n",
      "○​ Richer responses by extending knowledge \n",
      "○​ Ability to carry out actions within the application.  \n",
      "○​ Enhanced capabilities, like calculations.  \n",
      "●​ How it works :  \n",
      "○​ In a request to the LLM, specify available tools.  \n",
      "○​ The reply is either Text, or a request to run a tool.  \n",
      "○​ We run the tool and call the LLM with the results.  \n",
      " \n",
      "Common Examples of AI Tools :  \n",
      " \n",
      " \n",
      "What are Agents :  \n",
      "●​ Software entities that can autonomously perform tasks.  \n",
      "Common characteristics of an Agent :  \n",
      "●​ Autonomous \n",
      "●​ Goal-oriented  \n",
      "●​ Task specific  \n",
      "Designed to work as part of an Agent Framework to solve complex problems with \n",
      "limited human involvement.  \n",
      "●​ Memory / persistence.  \n",
      "●​ Decision-making / orchestration.  \n",
      "●​ Planning capabilities.  \n",
      "●​ Use of tools; potentially connecting to databases or the internet.  \n",
      " \n",
      "What are we about to do ?  \n",
      " \n",
      " \n",
      "●​ Image Generation : Use the OpenAI interface to generate images.  \n",
      "●​ Make Agents : Create Agents to generate sound and images for our store. \n",
      "●​ Make an Agent Framework : Teach our AI Assistant to speak and draw.  \n",
      "  \n",
      "What’s an Agent AI :  \n",
      "1. Breaking a complex problem into smaller steps, with multiple LLMs carrying out \n",
      "specialized tasks  \n",
      "2. The ability for LLMs to use Tools to give them additional capabilities  \n",
      "3. The 'Agent Environment' which allows Agents to collaborate  \n",
      "4. An LLM can act as the Planner, dividing bigger tasks into smaller ones for the \n",
      "specialists  \n",
      "5. The concept of an Agent having autonomy / agency, beyond just responding to a \n",
      "prompt - such as Memory \n",
      " \n",
      "HuggingFace Platform :  \n",
      "●​ The ubiquitous platform for LLM engineers :  \n",
      "○​ Models : Over 800,000 Open Source models of all shapes and sizes.  \n",
      "○​ Datasets : A treasure trove of 200,000 datasets.  \n",
      "○​ Spaces : Apps, many built in Gradio, including leaderboards.  \n",
      " \n",
      "It’s gives access to multiple libraries :  \n",
      "●​ Hub : Library allows you to login to a hugging face and both download and \n",
      "upload things like data sets and models.  \n",
      "●​ DataSets : Gives immediate access to data repositories in HuggingFace.  \n",
      "●​ Transformers : Provides wrapper code around LLMs that follow the transform \n",
      "architectures.  \n",
      "●​ PEFT (Parameter Efficient Fine Tuning) : Allows us to train LLMs without needing \n",
      "to work with all of the billions parameters.  \n",
      "●​ TRL (Transformer Reinforcement Learning) :  Ability to do things like reward \n",
      "modelling, supervised fine tuning.  \n",
      "●​ Accelerate : It allows both training and inference to run at scale in an efficient, \n",
      "adaptable way.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def upload_file(files) : \n",
    "    # Handle files if file_count = \"single\"\n",
    "    if(type(files) != list) : \n",
    "        summarize_content_pdf(files)\n",
    "        return files\n",
    "    # Handle files if file_count = \"multiple\"\n",
    "    else :\n",
    "      file_paths = [file.name for file in files]\n",
    "      summarize_content_pdf(file_paths[0])\n",
    "      return file_paths\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo : \n",
    "    file_upload = gr.File();\n",
    "    upload_button = gr.UploadButton(label=\"Click to Upload a File\", file_types=['.pdf'], file_count=\"single\");\n",
    "    upload_button.upload(upload_file, upload_button, file_upload);\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
